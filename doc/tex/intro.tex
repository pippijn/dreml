\chapter{Introduction}

Regular expressions are a key component of modern software development. Many
applications are nowadays inconceivable without the efficient pattern matching
provided by regular expression engines. Ever more clever optimisations and ever
more features are implemented in engines used by Perl, Java and many other
programming languages. Most programming languages have at least one engine
written in it, or have bindings to a popular C regex engine such as PCRE.

Most of these engines share a common deficiency: they are slow. Naturally, they
are fast for most common expressions, because that is what they were optimised
for, but for almost every regular expression engine, a class of expressions
exists for which the engine is slow. Slow, in this context, means that it takes
exponential execution time, and that means that even for relatively small
inputs, the time taken for a match can exceed a human lifetime.

C\# can compile regular expressions to CIL byte code, which is then compiled to
native code by the .NET Just-In-Time compiler. PCRE contains a JIT mode, as
well, but these optimisations, while very useful, only reduce the runtime by a
constant factor.

Why do these engines have such inefficient edge-cases? The reason is that all of
them implement a superset of regular languages. In order to implement these
non-regular expressions, backtracking algorithms are used.

The fact that these algorithms take exponential runtime make them unfeasible for
many applications, in particular those that allow untrusted users to enter
arbitrary expressions. Russ Cox developed RE2, an automata based regular
expression engine, for exactly this reason: Google code search allows web users
to enter regular expressions that run against a large amount of source code. An
engine with potentially exponential runtime would create a security risk.

Applications requiring linear execution time for pattern matching generally use
actual regular expressions, i.e. descriptions of regular languages, that can be
compiled to efficient matching automata.

There are several ways to construct such automata. A well-known method is
Thompson's construction algorithm, described in \cite{thompson}. The paper
mentions backtracking algorithms and explains how finite automata can improve
execution time from exponential to linear. It builds on ideas from Janusz
Brzozowski's, who, in his article \cite{brzozowski}, developed a formalism to
construct a finite automaton for a regular language by taking the left
derivative of the language with respect to its input.


\section{Key ideas}

This thesis presents an increment to the theory of regular expression matching.
In \cite{pdpat}, Sulzmann and Lu introduce pattern sub-matching with partial
derivatives, and in \cite{pdere}, Caron, Champarnaud and Mignot introduce a more
efficient method of extended regular expression matching. In particular, our
contributions are:

\begin{itemize}

   \item Pattern sub-matching based on partial derivatives with extended regular
      expressions.

   \item Clarification of some of the ideas presented in the papers this thesis
      is based on.

   \item A generic sub-matching engine based on partial derivatives and
      incorporating extended regular patterns.

   \item A lexer tool compatible with \ocamllex{} built with the generic engine.

\end{itemize}


\section{Efficient regex sub-matching}

The goal of this thesis is to create an efficient replacement for \ocamllex{}
based on the theory of partial derivatives. OCaml contains the \ocamllex{}
program, which is used in the implementation of the OCaml compiler itself, as
well as many other languages implemented in OCaml.  Unfortunately, this tool has
several serious drawbacks:

\begin{itemize}

   \item The automaton is restricted in size.

      It is not possible to list all C keywords in the lexer description, since
      the program will refuse to generate a large automaton.

   \item Pattern sub-matching can increase the automaton size considerably.

      When adding sub-matches such as \verb!(i:[0-9])(s:[lLuU]*)! for the C
      programming language, \ocamllex{} output increases in size. With many
      sub-match patterns, the tool can run into internal limits.

   \item Expression syntax is weak.

      There is no support for regular expression negation or intersection.
      Counted repetition is not supported. The accepted expression syntax is
      very basic.

   \item No compositing.

      This is not a serious drawback, but parser generators such as Menhir have
      supported grammar compositing for a long time. Lexical grammar compositing
      is particularly useful when creating a combined C/C++ front-end or for
      supporting language extensions such as GNU C keywords while keeping the
      base lexer definition clean and standard.

   \item Weak support for line number counting.

      In order to correctly track the line number in the lexer, the user must
      create a special rule in every sub-automaton where a new-line character
      may occur, just to call \texttt{Lexing.new\_line lexbuf}. This requirement
      makes the resulting lexer slower and the grammar more error-prone.

\end{itemize}

We developed a new tool, \reml{}, which is fully backwards compatible with
\ocamllex{}, but solves all of the above drawbacks.

\begin{itemize}

   \item The automaton size is unbounded.

      Our approach is based on non-deterministic finite automata, constructed
      from partial derivatives. The automaton size is therefore linear in the
      expression length. A lexer generated by \reml{} from a grammar twice as
      long will be twice as large. We can easily support thousands of keywords
      without the automaton growing to impossible sizes.

   \item Pattern sub-matching is fully integrated.

      Tagged transitions do not considerably increase the automaton size. There
      is no limit to the number of sub-matches. In fact, rules in \reml{} are
      implemented as sub-matches, internally.

   \item Richer expression syntax.

      The integration of extended regular expression syntax, as well as some
      other commonly found extensions, give \reml{} users the power to describe
      their lexical grammar in a more expressive way.

   \item Grammar compositing.

      \reml{} has direct support for importing definitions and even entire
      grammars from other files.

   \item Direct support for line numbers.

      Every transition on the new-line character automatically calls
      \texttt{Lexing.new\_line lexbuf}, so the user can omit these special
      rules, entirely. For backwards compatibility, this support can be
      disabled.

\end{itemize}


\section{Pattern sub-matching}

The C programming language specifies integer literals as a number followed by
zero or more type suffixes, \texttt{l} standing for \texttt{long}, \texttt{u}
for \texttt{unsigned}. In a C compiler, it is therefore useful to extract the
parts directly during the lexical analysis, in order to reduce duplication of
knowledge.

Perhaps an even better example for a case where pattern sub-matching is useful
is in C preprocessor directives, particularly the \verb!#include! directive. In
this, we are only interested in the name of the included file, so we will have a
pattern such as $\texttt{\#include } W^* \texttt{"} (f:(\Sigma \setminus
\{\texttt{"}\})^*) \texttt{"}$. Matching a valid include-directive with this
pattern will record the file name in the pattern variable $f$. E.g. considering
the input \verb!#include "stdio.h"!, the matching environment will consist of
the set $\Gamma = \{ (f:\texttt{stdio.h}) \}$. The file name can then be
extracted and used in a semantic action or post-processing step.

\subsection{Declarative lexer specifications}

Lexical analysers for programming languages are often described using domain
specific lexer specification languages. These consist of a list of regular
expressions associated with semantic actions, which are most often written in
the target language. Tools such as \texttt{lex} and derivatives for other
languages read these specifications and produce efficient matching automata from
them.

After a lexeme has been recognised, the lexer needs to call the appropriate user
code that will produce a token or send the information to an asynchronous
communication channel. In general, a lexer function receives an input stream
that reads characters until a single token has been successfully read and
returns. It must handle multiple regular expressions in parallel, and the
semantic action associated with the longest left-most match is executed.

Lexer tools often produce a large regular expression with all sub-expressions in
disjunction. In other words, all expressions $r_1 \dots r_n$ are put into a
unifying choice $(r_1 + \dots + r_n)$. The last final state and the stream
position are saved and restored when the automaton goes into the error state.
When the error state is reached, either by reading the end of file symbol or
because none of the regular expressions can continue with the symbol read, the
code associated with the last final state is executed and the stream position is
restored.

\cite{re-deriv} uses Brzozowski's regular vectors to handle multiple regular
expressions in the context of scanners when using regular expression
derivatives. This method does not create a disjunction over all expressions, but
rather preserves each of them in a vector, so that the expressions $r_1 \dots
r_n$ form the regular vector $(r_1, \dots, r_n)$. The derivative operation is
performed component-wise, so that $\dda(r_1, \dots, r_n) = (\dda r_1, \dots \dda
r_n)$. DFA states are labelled with regular vectors instead of regular
expressions. A final state is defined as a state labelled with a vector
containing at least one regular expression $r$ with $\varepsilon \in L(r)$, that
is, it accepts the empty word, and the error state is the one in which every
vector member $r$ represents the empty language, that is $L(r) = \emptyset$.

We can recognise that the goal achieved by using regular vectors is very similar
to sub-matching. In fact, the problem can easily be described in terms of
sub-matching. Using our generic sub-matching engine, we can elegantly declare a
scanner for lexeme patterns $(x_1:r_1) \dots (x_n:r_n)$ as a disjunction
$((x_1:r_1) + \dots + (x_n:r_n))$. The greedy left-most match then yields the
same result as the regular vector or the ad-hoc state switch approach.


\section{Extended regular expressions}

This thesis combines pattern sub-matching with extended regular expressions,
i.e. including the additional boolean operators $\neg$ for negation and $\cap$
for intersection. Expression negation is useful for C comments of the form
\verb!/* ... */!. A pattern matching C comments and extracting the comment text
may be written as $\texttt{/*}(x:\neg(\Sigma^*\texttt{*/}\Sigma^*))\texttt{*/}$.
Describing the same language without negation would require a longer
expression\footnote{Taken from \cite{re-deriv}.}:
\[
  \texttt{/*}
  (
    (\Sigma \setminus \{*\})^*
    (\varepsilon + \texttt{*}^* (\Sigma \setminus \{\texttt{/}, \texttt{*}\}))
  )^*
  \texttt{*/}
\]
Patterns involving more complex or more lengthly opening and closing expressions
would become much more cumbersome to express without negation.

Intersections are useful in subtracting regular expressions from patterns. One
could specify a base pattern for C integer literals, not including hexadecimal
literals, as $p_{\mathit{int}} = (\mathit{num}:(0 \dots 9)^n)
(\mathit{suf}:(l+L+u+U)^*)$, where $n \in \mathbb{N} \setminus \{0\}$, after
which \textit{num} contains the number and \textit{suf} the type suffix. Next,
we can restrict the pattern for octal integer literals by requiring it to begin
with a zero followed by anything not containing digits 8 or 9: $p_{\mathit{oct}}
= p_{\mathit{int}} \cap (0, \Sigma \setminus \{8, 9\})$.

This is particularly useful in the presence of composed regular grammars. A
library of standard regular expressions may define a set of valid C identifiers,
which may then be restricted in specialised lexers used to verify a coding
style or perform syntax highlighting based on coding conventions.


\section{Outline}

In chapter \ref{background}, we will explain the background for the ideas on
which our tool is based. First, we define a basic regular expression syntax,
which is later extended with additional boolean operators. Brzozowski's
derivatives will be outlined in the first section, which is followed by a short
introduction to Antimirov's partial derivatives. In the next section, pattern
sub-matching as done by Sulzmann and Lu in \cite{pdpat} is detailed. Finally, a
representation of partial derivatives as disjunctive normal form of terms as
described in \cite{pdere} is explained and shown by example.

The next chapter (\ref{submatching}) aims to combine the two ideas and
introduces extended pattern submatching. A pattern matching relation for
extended regular expression patterns is specified, and in compliance with that,
the partial derivative operation is defined. Section \ref{nfa} uses this
definition to provide a pattern partial derivative function compatible with the
one in \cite{pdpat}, and extended by the syntax of \cite{pdere}. It also
explains how this function can be used to construct a finite state machine from
a pattern. At the end of this chapter, a language predicate optimisation is
defined in order to reduce the number of unreachable states produced by the
automata construction.

Chapter \ref{implementation} describes the implementation details of the generic
sub-matching engine, which does not support all of \reml's input syntax, but
implements all the required algorithms. The prototype first implemented
Brzozowski's derivatives, then Antimirov's partial derivatives, which were
enhanced by Sulzmann and Lu's pattern sub-matching and finally extended with
intersection and negation. This development process will be followed and
peculiarities pointed out. After the basic implementation has been detailed,
some of the more important optimisations will be described.

Following the implementation, chapter \ref{results} presents empirical results
in the form of timing and comparison with existing tools.

Chapter \ref{discussion} discusses the results of this thesis, as well as some
results of related work. It draws the distinction between the presented work and
previous results.

The conclusion in chapter \ref{conclusion}...


% vim:tw=80
